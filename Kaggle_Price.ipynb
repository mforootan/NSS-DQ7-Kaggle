{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading CSV files\n",
    "\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "train_data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should remove NANs from a list\n",
    "def cleanan(input_list):\n",
    "    cleaned = [i for i in input_list if str(i) != 'nan']\n",
    "    return (cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train Data\n",
    "\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()[train_data.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Test Data\n",
    "\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()[test_data.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we split numeric and non-numeric data into to separate dataframes to deal with NANs separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train Data\n",
    "\n",
    "cols_to_change_1 = train_data.select_dtypes(exclude=['float64','int64'])\n",
    "cols_to_keep_1 = train_data.select_dtypes(include=['float64','int64'])\n",
    "\n",
    "# indices should be added for future merge\n",
    "\n",
    "cols_to_change_1['IDX']=cols_to_change_1.index\n",
    "cols_to_keep_1['IDX']=cols_to_keep_1.index\n",
    "\n",
    "# 2. Test Data\n",
    "\n",
    "cols_to_change_2 = test_data.select_dtypes(exclude=['float64','int64'])\n",
    "cols_to_keep_2 = test_data.select_dtypes(include=['float64','int64'])\n",
    "\n",
    "# indices should be added for future merge\n",
    "\n",
    "cols_to_change_2['IDX']=cols_to_change_2.index\n",
    "cols_to_keep_2['IDX']=cols_to_keep_2.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Visual inspection of non-numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.TRAIN DATA\n",
    "non_num=[]\n",
    "\n",
    "for i in range(1, cols_to_change_1.shape[1]):\n",
    "    non_num.append([cols_to_change_1.columns[i], \n",
    "                    cols_to_change_1[cols_to_change_1.columns[i]].isnull().sum(), \n",
    "                    cols_to_change_1[cols_to_change_1.columns[i]].unique(),\n",
    "                   cleanan(cols_to_change_1[cols_to_change_1.columns[i]].unique())]) \n",
    "    \n",
    "\n",
    "df_str = pd.DataFrame(non_num)\n",
    "df_str.columns=['col','NANs','Unique','Unique_Clean']\n",
    "\n",
    "\n",
    "df_str.sort_values(by='NANs', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TEST DATA\n",
    "non_num=[]\n",
    "\n",
    "for i in range(1, cols_to_change_2.shape[1]):\n",
    "    non_num.append([cols_to_change_2.columns[i], \n",
    "                    cols_to_change_2[cols_to_change_2.columns[i]].isnull().sum(), \n",
    "                    cols_to_change_2[cols_to_change_2.columns[i]].unique(),\n",
    "                   cleanan(cols_to_change_2[cols_to_change_2.columns[i]].unique())]) \n",
    "    \n",
    "\n",
    "df_str = pd.DataFrame(non_num)\n",
    "df_str.columns=['col','NANs','Unique','Unique_Clean']\n",
    "\n",
    "\n",
    "df_str.sort_values(by='NANs', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The columns in this loop have a large number of NANs. \n",
    "##### Instead of replacing them with most frequent, a random value of non-NANs with be substituted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TRAIN DATA\n",
    "\n",
    "\n",
    "for colu in ['Alley','FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']:\n",
    "    cols_to_change_1[colu]=cols_to_change_1[colu].apply(lambda v: np.random.choice(cleanan(cols_to_change_1[colu].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TEST DATA\n",
    "\n",
    "\n",
    "for colu in ['Alley','FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']:\n",
    "    cols_to_change_2[colu]=cols_to_change_2[colu].apply(lambda v: np.random.choice(cleanan(cols_to_change_2[colu].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the rest of the columns, NANs will be replaced with most frequent value\n",
    "#### No need to filter non-NAN columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TRAIN DATA\n",
    "\n",
    "for colu in cols_to_change_1.columns:\n",
    "    cols_to_change_1[colu].fillna(cols_to_change_1[colu].mode()[0], inplace = True)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TEST DATA\n",
    "\n",
    "for colu in cols_to_change_2.columns:\n",
    "    cols_to_change_2[colu].fillna(cols_to_change_2[colu].mode()[0], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking for any NAN left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TRAIN DATA\n",
    "\n",
    "cols_to_change_1.isnull().sum()[cols_to_change_1.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TEST DATA\n",
    "\n",
    "cols_to_change_2.isnull().sum()[cols_to_change_2.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dummies values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. TRAIN DATA\n",
    "\n",
    "cols_to_change_dum_1 = pd.get_dummies(cols_to_change_1, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. TEST DATA\n",
    "\n",
    "cols_to_change_dum_2 = pd.get_dummies(cols_to_change_2, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual inspection of numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TRAIN DATA\n",
    "\n",
    "num_vals=[]\n",
    "for i in range(1, cols_to_keep_1.shape[1]):\n",
    "    num_vals.append([cols_to_keep_1.columns[i], cols_to_keep_1[cols_to_keep_1.columns[i]].mean(), cols_to_keep_1[cols_to_keep_1.columns[i]].median(), cols_to_keep_1[cols_to_keep_1.columns[i]].isnull().sum()])\n",
    "\n",
    "df_num = pd.DataFrame(num_vals)\n",
    "df_num.columns=['col','mean','median','NANs']\n",
    "\n",
    "df_num.sort_values(by='NANs', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TEST DATA\n",
    "\n",
    "num_vals=[]\n",
    "for i in range(1, cols_to_keep_2.shape[1]):\n",
    "    num_vals.append([cols_to_keep_2.columns[i], cols_to_keep_2[cols_to_keep_2.columns[i]].mean(), cols_to_keep_2[cols_to_keep_2.columns[i]].median(), cols_to_keep_2[cols_to_keep_2.columns[i]].isnull().sum()])\n",
    "\n",
    "df_num = pd.DataFrame(num_vals)\n",
    "df_num.columns=['col','mean','median','NANs']\n",
    "\n",
    "df_num.sort_values(by='NANs', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing the NaN values with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. TRAIN DATA\n",
    "\n",
    "imp = Imputer(missing_values = 'NaN', strategy = 'median', axis =0)\n",
    "imp.fit(cols_to_keep_1)\n",
    "cols_to_keep_1_nonan = imp.transform(cols_to_keep_1)\n",
    "\n",
    "# Convert into dataframe\n",
    "cols_to_keep_1_df = pd.DataFrame(cols_to_keep_1_nonan)\n",
    "\n",
    "# the new dataframe has no column header, so copy it from the main one\n",
    "cols_to_keep_1_df.columns = cols_to_keep_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. TEST DATA\n",
    "\n",
    "imp = Imputer(missing_values = 'NaN', strategy = 'median', axis =0)\n",
    "imp.fit(cols_to_keep_2)\n",
    "cols_to_keep_2_nonan = imp.transform(cols_to_keep_2)\n",
    "\n",
    "# Convert into dataframe\n",
    "cols_to_keep_2_df = pd.DataFrame(cols_to_keep_2_nonan)\n",
    "\n",
    "# the new dataframe has no column header, so copy it from the main one\n",
    "cols_to_keep_2_df.columns = cols_to_keep_2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the numeric and non-numeric parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Train Data\n",
    "new_train = pd.merge(cols_to_change_dum_1, cols_to_keep_1_df, how='left', on='IDX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Test Data\n",
    "new_test = pd.merge(cols_to_change_dum_2, cols_to_keep_2_df, how='left', on='IDX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oooops! Some missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find what columns are missing in test data\n",
    "discrep = [value for value in new_train.columns if not(value in new_test.columns)]\n",
    "discrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward check\n",
    "discrep2 = [value for value in new_test.columns if not(value in new_train.columns)]\n",
    "discrep2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a 'blank' dataframe using the missing columns PLUS column 'IDX' for merging ref\n",
    "new_test_cols = pd.DataFrame(columns=discrep)\n",
    "new_test_cols['IDX']=new_test.IDX\n",
    "new_test_cols.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_test_adjusted = pd.merge(new_test, new_test_cols, how='left', on='IDX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_adjusted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check - Now the train and test dataset are the same.\n",
    "discrep = [value for value in new_train.columns if not(value in new_test_adjusted.columns)]\n",
    "discrep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "#### Vinay's notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a correlation \n",
    "corrmat = new_train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the correlation coffeccient among the variables\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(new_train[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the unique values in overall quality\n",
    "new_train.OverallQual.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot overallqual/saleprice\n",
    "var = 'OverallQual'\n",
    "data = pd.concat([new_train['SalePrice'], new_train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot Kitchenqual/saleprice\n",
    "var = 'KitchenQual'\n",
    "data = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot Garagequal/saleprice\n",
    "var = 'GarageQual'\n",
    "data = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot RoofMatl/saleprice\n",
    "var = 'RoofMatl'\n",
    "data = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot LandSlope/saleprice\n",
    "var = 'LandSlope'\n",
    "data = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot Street/saleprice\n",
    "var = 'Street'\n",
    "data = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot Neighborhood/saleprice\n",
    "var = 'WoodDeckSF'\n",
    "data = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "sns.set()\n",
    "cols = ['SalePrice', 'YearRemodAdd', 'OverallQual', '1stFlrSF', 'GarageCars', 'TotalBsmtSF', 'FullBath']\n",
    "sns.pairplot(train_data[cols], size = 3)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking some outliers in the data using scatter plot\n",
    "plt.scatter(train_data.GrLivArea, train_data.SalePrice, c = \"blue\", marker = \"s\")\n",
    "plt.title(\"Looking for outliers\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()\n",
    "# We can get rid of living area more than 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking some outliers in the data using scatter plot\n",
    "plt.scatter(train_data.OverallQual, train_data.SalePrice, c = \"blue\", marker = \"s\")\n",
    "plt.title(\"Looking for outliers\")\n",
    "plt.xlabel(\"OverallQual\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()\n",
    "# Looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking some outliers in the data using scatter plot\n",
    "plt.scatter(train_data[\"1stFlrSF\"], train_data.SalePrice, c = \"blue\", marker = \"s\")\n",
    "plt.title(\"Looking for outliers\")\n",
    "plt.xlabel(\"1stFlrSF\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()\n",
    "# anything greater than 3500 looks fishy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking some outliers in the data using scatter plot\n",
    "plt.scatter(train_data[\"TotalBsmtSF\"], train_data.SalePrice, c = \"blue\", marker = \"s\")\n",
    "plt.title(\"Looking for outliers\")\n",
    "plt.xlabel(\"TotalBsmtSF\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()\n",
    "# anything greater than 3500 looks fishy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking some outliers in the data using scatter plot\n",
    "plt.scatter(train_data[\"FullBath\"], train_data.SalePrice, c = \"blue\", marker = \"s\")\n",
    "plt.title(\"Looking for outliers\")\n",
    "plt.xlabel(\"FullBath\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()\n",
    "# anything greater than 3500 looks fishy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My analysis starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_model = XGBRegressor()\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(X, y, verbose=False)\n",
    "\n",
    "# make predictions\n",
    "y_pred = my_model.predict(X_test.as_matrix())\n",
    "\n",
    "submission = pd.DataFrame({'Id': df_test.Id, 'SalePrice': y_pred})\n",
    "submission.to_csv('NSS_twisted_horses_XGB.csv', index=False)\n",
    "#Your submission scored 0.16555, which is an improvement of your previous score of 0.17572. Great job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********** DO NOT RUN UNTIL XGB SOLVED ****************\n",
    "\n",
    "my_pipeline = make_pipeline(Imputer(), XGBRegressor())\n",
    "\n",
    "my_pipeline.fit(X, y)\n",
    "y_pred = my_pipeline.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'Id': df_test.Id, 'SalePrice': y_pred})\n",
    "submission.to_csv('NSS_twisted_horses_pipeline_XGB.csv', index=False)\n",
    "#Your submission scored 0.18125, which is not an improvement of your best score. Keep trying!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create arrays for the features and the response variable\n",
    "y = new_train['SalePrice'].values\n",
    "X = new_train.drop('SalePrice', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "# Create the regressor: reg_all\n",
    "reg_all = LinearRegression()\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "reg_all.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data: y_pred\n",
    "y_pred = reg_all.predict(X_test)\n",
    "\n",
    "# Compute and print R^2 and RMSE\n",
    "print(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit = [reg_all.predict(X)]\n",
    "pd.DataFrame(y_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute 5-fold cross-validation scores: cv_scores\n",
    "cv_scores = cross_val_score(reg_all, X, y, cv=5)\n",
    "\n",
    "# Print the 5-fold cross-validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a lasso regressor: lasso\n",
    "lasso = Lasso(alpha=0.4, normalize=True)\n",
    "\n",
    "# Fit the regressor to the data\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Compute and print the coefficients\n",
    "lasso_coef = lasso.coef_\n",
    "print(lasso_coef)\n",
    "\n",
    "# Plot the coefficients\n",
    "plt.plot(range(len(new_train)), lasso_coef)\n",
    "plt.xticks(range(len(new_train)), new_train.values, rotation=60)\n",
    "plt.margins(0.02)\n",
    "plt.show()\n",
    "\n",
    "# ValueError: x and y must have same first dimension, but have shapes (1460,) and (247,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-4, 0, 50)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "# Create a ridge regressor: ridge\n",
    "ridge = Ridge(alpha=0.1, normalize=True)\n",
    "\n",
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: ridge.alpha\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))\n",
    "\n",
    "# Display the plot\n",
    "# print(\"RS:\",ridge_scores, \"RS_std:\",ridge_scores_std)\n",
    "_ = plt.scatter(ridge_scores, ridge_scores_std, c = \"blue\", marker = \".\")\n",
    "_ = plt.title(\"Ridge Score\")\n",
    "_ = plt.xlabel(\"Ridge Score\")\n",
    "_ = plt.ylabel(\"Standardized RS\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "\n",
    "# Instantiate a k-NN classifier: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
